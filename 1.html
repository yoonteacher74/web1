<!doctype html>
<html>
<head>
  <title>프로그래밍 언어</title>
  <meta charset="utf-8">
</head>
<body>
  <h1><a href="index.html">프로그래밍 언어</a></h1>
  <ol>
    <li><a href="1.html">등장배경과 역사</a></li>
    <li><a href="2.html">구조</a></li>
    <li><a href="3.html">분류(종류)</a></li>
  </ol>
  <h2>등장배경과 역사</h2>
<p>역사를 잘 살펴보면 프로그래밍 비스무리한것을 만들고 했던 기인들이 종종 등장하지만, 프로그래밍 언어라는 본격적인 개념은 역시 수학에서 등장하였다. 쿠르트 괴델은 불완전성 정리를 증명하는 과정중에 알고리즘을 추상화시킨 원시 재귀 함수(primitive recursive function) 개념을 만들고, 이를 이용하여 증명에 성공하였는데, 수학적으로 본다면 이것이 최초의 프로그래밍 언어라 볼 수 있다.(굳이 따지자면 함수형 언어라 할 수 있다.) 그리고, 몇년 후에 컴퓨터의 아버지라 불리는 튜링은 불완전성 정리를 보고 "어 이거 내 방식대로 증명해볼 수 있겠는데?"라고 생각하며 연구를 하는데, 여기서 이 원시재귀함수와 동치인 튜링 머신을 발표하고 이 튜링머신을 이용하여 불완전성 정리를 다시 한 번 증명해보인다. 이는 "어떤 체계를 통해서 이 체계의 모순성을 증명할 수 있는 방법은 없다"와 동치이다.</p>
<p>그 이후 계산가능성이론(Computability Theory)이라는 수학의 분야가 생기면서 기존의 원시재귀함수를 확장한 Lambda calculus라거나 Unlimited register machine(URM), While-programming, SKI 등등 알고리즘을 표현하기 위한 여러가지 체계들이 등장한다.</p>
<p>참고로, Lambda calculus를 고안한 알론조 처치의 경우 혹시나 이것을 이용하면 수학의 완전성을 증명할 수 있지 않을까 해서 시도하지만, 실패한다. 문제는, 상기된 알고리즘 체계들은 모든 알고리즘을 다 표현할 수 있는 체계인가라는 의문인데, 알고리즘이라는것이 수학적으로 정의된 개념이 아니기때문에 그것을 증명하는것은 불가능하였다. 하지만 저 모든 알고리즘 체계들이 표현방식은 다르더라도 수학적으로 볼때는 다 동일하였기 때문에 알론조 처치는 저 알고리즘 체계들이 '모든 알고리즘의 집합'과 동치라고 그냥 간주할 것을 제안하며, 이것이 그 유명한 처치의 명제(Church's Thesis)이다.</p>
<p>이렇게 수학 쪽에서 프로그래밍 언어에 대한 개념적인 기초가 닦아지는 동안, 역시 수학자인 폰 노이만은 그것들을 이용하여 실제 컴퓨터를 만들기 위한 폰 노이만 구조를 만든다. 그렇게 전자쪽과 결합을 하면서 현대 컴퓨터의 원형이라고 할 수 있는 물건이 만들어졌다. 이러한 전자적인 계산장치는 전기신호를 통해서 제어하였는데, 전기신호를 표현할 수 있는 방법은 신호가 들어왔다(1), 신호가 들어오지 않았다(0) 정도에 불과하였다. 따라서 제어신호는 0과 1만으로 표현하는 라이프니츠[5]식 2진법을 사용할 수 밖에 없었고, 특정한 패턴의 전기신호는 어떠한 동작을 의미한다는 식으로 사람들이 정하고 전자계산장치는 그 신호가 입력되면 정해진 동작을 하는 형태였다.하지만 실제 컴퓨터의 동작에서는 장치가 꺼져서 전기신호가 없는 상태인지 아니면 0을 나타내는 상태인지 구별하기 위하여 0은 기준 레벨(일반적으로 볼트 단위로 표시한다) 이하의 신호일 경우 0으로 본다. 마찬가지로 1 역시 기준 레벨 이상의 신호일 경우 1로 본다.
따라서 뭔가 동작을 시키기 위해서는 이러한 제어신호를 사람이 직접 작성해야 됐는데, 초창기에는 그 0과 1의 제어신호를 사람이 직접 작성하는 형태의 기계어가 사용되었다. 이 기계어는 항목에서 볼 수 있듯이 이쪽 분야에서 가장 원시적인 언어로 기계는 바로 이해할 수 있는데 사람은 도저히 이해하기도 어렵고 알아보기도 힘든 물건이라 결국 사람이 읽기 편하도록 기계어와 특정 기호를 1:1로 대응시키는 어셈블리어가 등장하였다. 과거 기계어를 쓰는 시절보다는 보기가 좀 편해졌지만 여전히 해독하기가 난해하였고, 컴퓨터의 보급과 함께 프로그램의 수요가 늘어나는데 어셈블리어의 생산성은 심히 안습이라서 조금 더 프로그램을 짜기 쉬운 언어들이 등장하기 시작하였다.</p>
<p>초창기에 프로그래밍 언어들은 컴퓨터 성능의 한계로 인해 많은 제약이 따라붙었고, 어셈블리어적인 성격이 어느정도 남아있었다. 그럼에도 어셈블리어에 비하면 읽기가 편했고 이해하기도 훨씬 수월했다. 그리고 컴퓨터의 보급과 성능 발달에 맞물려 그동안 걸려있던 제약조건들도 하나씩 사라지고 보다 사람이 읽기도 쉽고, 이해하기도 쉽고, 작성하기도 쉬운 프로그래밍 언어들이 속속 등장하였다.
다만 어차피 컴퓨터가 이해할 수 있는 언어는 기계어 뿐이기 때문에 사람이 하기에 편해졌다는 것 뿐이지 실제 그 뒤에서 이루어지는 작업은 훨씬 더 복잡해지고 있다.</p>
<p>정리하자면 기계에게 편한 언어는 속도가 빠르지만 사람이 도저히 알아볼 수 있는 물건이 아니라서 생산성이 떨어진다. 반면 사람에게 편한 언어는 속도는 느리지만 이해하기 쉽고 생산성이 높다는 상관관계가 성립한다. 회화 언어로 치자면 중역의 문제로 볼 수 있다.</P>
<p>단, 여기에 함정이 있는데 기계어로 짠 발적화 코드와 자바로 짠 최적화 코드의 수행속도를 비교한다면 당연히 자바 쪽이 빠르다. 그러니까 기계어를 사용한다 해도 그걸 다루는 프로그래머가 기계의 특성을 훤히 꿰고 있지 않으면 다른 고급언어를 사용한 결과물보다 느려질 수도 있다는 얘기다. 그래서 몇몇 개발관련 서적에서는 최적화나 성능 튜닝한답시고 기계어나 어셈블리어를 남용하지 말라고 조언한다. 실제로도, 지금은 컴파일러와 어셈블러가 무척 발달되어 있어서 사람이 어설프게 짠 코드보다 기계가 변환한 코드 쪽이 더 낫다. 물론 기계가 알고리즘 자체를 개선해주지는 못하므로 컴파일러한테 자신이 의도한 바를 명확하게 전달하는 게 더 중요하다. 예를 들어 "이 배열에 있는 값은 계산 끝날 때까지 중간에 바뀔 일은 절대 없으며 숫자 말고 엉뚱한 건 전혀 없다고 보장하고, 하고 싶은 일은 이걸 다 더하는 것이다." 라고 컴파일러에게 명확하게 자신의 의도를 전달하면 컴파일러는 각종 하드웨어 가속과 코드 병렬화 등을 수행해 프로그래머가 의도한 일을 자신이 할 수 있는 가장 빠른 방법으로 구현해낸다. 일반 루프문을 사용한 배열 숫자 덧셈은 컴파일러가 숫자가 중간에 바뀔지 안바뀔지 판단을 못 하는 경우(전역변수의 배열을 더한다거나)가 생길 수 있는데 애초에 이런 코드를 만들지 않는 능력이 더 중요하다.</P>
  <h3>참고링크</h3>
</p>
<li><a href="https://codedragon.tistory.com/4009">Code Dragon</li>
<li><a href="http://pchero21.com/?p=717">탱이의 세상만사</li>
</p>
</body>
</html>
